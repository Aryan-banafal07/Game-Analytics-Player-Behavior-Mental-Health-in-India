# -*- coding: utf-8 -*-
"""DE-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_-YMermWd4M4pfCeiexQEBr0Ro0uzDf_
"""

# Data Preprocessing for Player Mental Health & Gaming Behavior Dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer


# Load dataset
dataset = pd.read_excel('player_data_1000_entries.xlsx')

# Handling missing data
imputer = SimpleImputer(strategy='mean')
dataset[['Time_Spent_Per_Session (mins)', 'Sessions_Per_Day', 'Total_Hours_Per_Week', 'Sleep_Hours']] = imputer.fit_transform(
    dataset[['Time_Spent_Per_Session (mins)', 'Sessions_Per_Day', 'Total_Hours_Per_Week', 'Sleep_Hours']])


# Splitting 'Mental_Health' into separate columns (Stress, Anxiety, Depression, Overall_MH)
mh_columns = ['Stress', 'Anxiety', 'Depression', 'Overall_MH']
dataset[mh_columns] = dataset['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'].str.split('/', expand=True)
dataset.drop('Mental_Health (Stress/Anxiety/Depression/Overall_MH)', axis=1, inplace=True)


# Convert mental health columns to numeric
dataset[mh_columns] = dataset[mh_columns].apply(pd.to_numeric, errors='coerce')

# One-hot encoding for categorical columns (Gender, Occupation, Game_Name, Platform)
categorical_cols = ['Gender', 'Occupation', 'Game_Name', 'Platform']
dataset = pd.get_dummies(dataset, columns=categorical_cols, drop_first=True)

# Feature Scaling (MinMax scaling for better comparison between time spent, sessions, and mental health)
scaler = MinMaxScaler()
dataset[['Time_Spent_Per_Session (mins)', 'Sessions_Per_Day', 'Total_Hours_Per_Week', 'Sleep_Hours', 'Stress', 'Anxiety', 'Depression', 'Overall_MH']] = scaler.fit_transform(
    dataset[['Time_Spent_Per_Session (mins)', 'Sessions_Per_Day', 'Total_Hours_Per_Week', 'Sleep_Hours', 'Stress', 'Anxiety', 'Depression', 'Overall_MH']])


# Defining features and target variable (assuming target could be any one of the mental health columns)
X = dataset.drop(columns=['Player_ID', 'Overall_MH'])  # Dropping Player_ID and assuming 'Overall_MH' as target
y = dataset['Overall_MH']

# Splitting the dataset into Training and Test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)




# Output data for verification
print(X_train.head())
print(y_train.head())




# Visualization
# 1. Histogram of Time Spent Per Session
plt.figure(figsize=(8, 6))
plt.hist(dataset['Time_Spent_Per_Session (mins)'], bins=30, color='skyblue', edgecolor='black')
plt.title('Distribution of Time Spent Per Session (mins)')
plt.xlabel('Time Spent (mins)')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# 2. Histogram of Total Hours Per Week
plt.figure(figsize=(8, 6))
plt.hist(dataset['Total_Hours_Per_Week'], bins=30, color='lightgreen', edgecolor='black')
plt.title('Distribution of Total Hours Played Per Week')
plt.xlabel('Total Hours')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

# 3. Boxplot for Mental Health Metrics (Stress, Anxiety, Depression)
plt.figure(figsize=(10, 6))
dataset.boxplot(column=['Stress', 'Anxiety', 'Depression'])
plt.title('Boxplot of Mental Health Metrics')
plt.ylabel('Scaled Score')
plt.grid(True)
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.impute import SimpleImputer
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load the dataset
file_path = 'player_data_encoded_fixed.xlsx'  # Update the file path if necessary
data = pd.read_excel(file_path)

# Step 2: Check for problematic columns by inspecting unique values in each column
for col in data.columns:
    print(f"{col}: {data[col].unique()}")

# Step 3: Clean or encode necessary columns
data['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'] = data['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'].map({
    '4/5/3/Fair': 1,
    '4/6/4/Poor': 2,
    '5/7/5/Excellent': 0,
    '5/6/2/Good': 3
})

# Step 4: Select features and target
features = data.drop(columns=['Player_ID', 'Overall_MH', 'Overall_MH_Encoded'])
target = data['Overall_MH_Encoded']

# Step 5: One-hot encode categorical variables
features_encoded = pd.get_dummies(features, columns=['Gender', 'Occupation', 'Game_Name', 'Platform'], drop_first=True)

# Step 6: Handle missing values
# Option 1: Impute missing values (using mean for numerical data)
imputer = SimpleImputer(strategy='mean')  # Change strategy as needed (median, most_frequent, etc.)
features_encoded_imputed = pd.DataFrame(imputer.fit_transform(features_encoded), columns=features_encoded.columns)

# Option 2: Alternatively, you could drop rows with missing values
# features_encoded_imputed = features_encoded.dropna()

# Step 7: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features_encoded_imputed, target, test_size=0.2, random_state=42)

# Step 8: Initialize and train RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Step 9: Initialize and train Logistic Regression
lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train, y_train)

# Step 10: Initialize and train Support Vector Classifier
svm_model = SVC(probability=True)
svm_model.fit(X_train, y_train)

# Step 11: Make predictions for all models
rf_pred = rf_model.predict(X_test)
lr_pred = lr_model.predict(X_test)
svm_pred = svm_model.predict(X_test)

# Step 12: Evaluate each model
def evaluate_model(model_name, y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    report = classification_report(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)

    print(f"Model: {model_name}")
    print(f"Accuracy: {accuracy:.2f}")
    print("Classification Report:")
    print(report)

    # Plot confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.ylabel('Actual Class')
    plt.xlabel('Predicted Class')
    plt.show()

# Evaluate all models
evaluate_model("Random Forest", y_test, rf_pred)
evaluate_model("Logistic Regression", y_test, lr_pred)
evaluate_model("Support Vector Classifier", y_test, svm_pred)

# Step 13: Make predictions for all users with Random Forest
data['Predicted_MH_Encoded'] = rf_model.predict(features_encoded_imputed)

# Step 14: Identify and List High-Risk Users
high_risk_users = data[data['Predicted_MH_Encoded'] == 1]
print("High Risk Users:")
print(high_risk_users[['Player_ID', 'Predicted_MH_Encoded']])

# Save the high-risk users list to a CSV file (optional)
high_risk_users.to_csv('high_risk_users.csv', index=False)

# Step 15: Feature Importance from Random Forest
feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf_model.feature_importances_})
feature_importances = feature_importances.sort_values('Importance', ascending=False)

# Step 16: Plot feature importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importance from Random Forest')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load the dataset
file_path = 'player_data_encoded_fixed.xlsx'  # Update the file path if necessary
data = pd.read_excel(file_path)

# Step 2: Clean or encode necessary columns
data['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'] = data['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'].map({
    '4/5/3/Fair': 1,
    '4/6/4/Poor': 2,
    '5/7/5/Excellent': 0,
    '5/6/2/Good': 3
})

# Step 3: Select features and target
features = data.drop(columns=['Player_ID', 'Overall_MH', 'Overall_MH_Encoded'])
target = data['Overall_MH_Encoded']

# Step 4: One-hot encode categorical variables
features_encoded = pd.get_dummies(features, columns=['Gender', 'Occupation', 'Game_Name', 'Platform'], drop_first=True)

# Step 5: Handle missing values
imputer = SimpleImputer(strategy='mean')
features_encoded_imputed = pd.DataFrame(imputer.fit_transform(features_encoded), columns=features_encoded.columns)

# Step 6: Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(features_encoded_imputed, target)

# Step 7: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Step 8: Initialize and train Gradient Boosting Classifier with Grid Search for Hyperparameter Tuning
gb_model = GradientBoostingClassifier(random_state=42)

# Set up parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='accuracy')
grid_search.fit(X_train, y_train)

# Best model from grid search
best_gb_model = grid_search.best_estimator_

# Step 9: Make predictions
y_pred = best_gb_model.predict(X_test)

# Step 10: Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Display results
print(f'Best Gradient Boosting Model Parameters: {grid_search.best_params_}')
print(f'Accuracy: {accuracy:.2f}')
print('Classification Report:')
print(report)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plotting the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Gradient Boosting')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

# Step 11: Make predictions for all users with the best Gradient Boosting model
data['Predicted_MH_Encoded'] = best_gb_model.predict(features_encoded_imputed)

# Step 12: Identify and List High-Risk Users
high_risk_users = data[data['Predicted_MH_Encoded'] == 1]
print("High Risk Users:")
print(high_risk_users[['Player_ID', 'Predicted_MH_Encoded']])

# Save the high-risk users list to a CSV file (optional)
high_risk_users.to_csv('high_risk_users.csv', index=False)





import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

# Step 1: Load the dataset using read_excel
file_path = 'player_data_encoded_fixed.xlsx'
data = pd.read_excel(file_path)

# Step 2: Sample a larger subset for better execution
data_sample = data.sample(frac=0.3, random_state=42)  # Increase fraction

# Step 3: Map the 'Mental_Health (Stress/Anxiety/Depression/Overall_MH)' column values
data_sample['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'] = data_sample['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'].map({
    '4/5/3/Fair': 1,
    '4/6/4/Poor': 2,
    '5/7/5/Excellent': 0,
    '5/6/2/Good': 3
})

# Step 4: Select features and target
features = data_sample.drop(columns=['Player_ID', 'Overall_MH', 'Overall_MH_Encoded'])
target = data_sample['Overall_MH_Encoded']

# Step 5: One-hot encode categorical variables
features_encoded = pd.get_dummies(features, columns=['Gender', 'Occupation', 'Game_Name', 'Platform'], drop_first=True)

# Step 6: Drop columns with all missing values
features_encoded = features_encoded.dropna(axis=1, how='all')

# Step 7: Separate numeric and non-numeric columns for imputation
numeric_cols = features_encoded.select_dtypes(include=['float64', 'int64']).columns
non_numeric_cols = features_encoded.select_dtypes(exclude=['float64', 'int64']).columns

# Impute only numeric columns
imputer = SimpleImputer(strategy='mean')
features_numeric_imputed = pd.DataFrame(imputer.fit_transform(features_encoded[numeric_cols]), columns=numeric_cols)

# Combine non-imputed non-numeric columns with imputed numeric columns
features_imputed = pd.concat([features_numeric_imputed, features_encoded[non_numeric_cols].reset_index(drop=True)], axis=1)

# Step 8: Feature scaling
scaler = StandardScaler()
features_scaled = pd.DataFrame(scaler.fit_transform(features_imputed), columns=features_imputed.columns)

# Step 9: Add Polynomial Features for potential non-linear relationships (higher degree)
poly = PolynomialFeatures(degree=2, include_bias=False)  # Increase degree
features_poly = pd.DataFrame(poly.fit_transform(features_scaled), columns=poly.get_feature_names_out(features_scaled.columns))

# Step 10: Handle class imbalance using SMOTE
# Analyze class distribution
class_counts = Counter(target)
print(f'Original class distribution: {class_counts}')

# Define SMOTE with multi-class sampling strategy
smote = SMOTE(sampling_strategy='auto', random_state=42)

# Apply SMOTE
X_resampled, y_resampled = smote.fit_resample(features_poly, target)

# Analyze new class distribution
new_class_counts = Counter(y_resampled)
print(f'Resampled class distribution: {new_class_counts}')

# Step 11: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Step 12: Initialize and train Gradient Boosting Classifier with improved parameter tuning
gb_model = GradientBoostingClassifier(random_state=42)

# Extended parameter grid for tuning
param_dist = {
    'n_estimators': [100, 200, 300, 500, 1000],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 9, 11],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 3, 4],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'max_features': ['sqrt', 'log2', None]
}

# Increase iterations for RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=gb_model,
    param_distributions=param_dist,
    n_iter=100,  # Further increase iterations
    cv=3,
    n_jobs=-1,
    scoring='accuracy',
    random_state=42,
    verbose=1
)

# Fit the model
random_search.fit(X_train, y_train)

# Best model from RandomizedSearchCV
best_gb_model = random_search.best_estimator_

# Step 13: Make predictions and evaluate the model
y_pred = best_gb_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print(f'Best Gradient Boosting Model Parameters: {random_search.best_params_}')
print(f'Accuracy: {accuracy:.2f}')
print('Classification Report:')
print(report)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plotting the Confusion Matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix for Gradient Boosting')
plt.ylabel('Actual Class')
plt.xlabel('Predicted Class')
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Step 1: Load the dataset
file_path = 'player_data_encoded_fixed.xlsx'  # Update the file path if necessary
data = pd.read_excel(file_path)

# Step 2: Clean or encode necessary columns
data['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'] = data['Mental_Health (Stress/Anxiety/Depression/Overall_MH)'].map({
    '4/5/3/Fair': 1,
    '4/6/4/Poor': 2,
    '5/7/5/Excellent': 0,
    '5/6/2/Good': 3
})

# Step 3: Select features and target
features = data.drop(columns=['Player_ID', 'Overall_MH', 'Overall_MH_Encoded'])
target = data['Overall_MH_Encoded']

# Step 4: One-hot encode categorical variables
features_encoded = pd.get_dummies(features, columns=['Gender', 'Occupation', 'Game_Name', 'Platform'], drop_first=True)

# Step 5: Handle missing values
imputer = SimpleImputer(strategy='mean')
features_encoded_imputed = pd.DataFrame(imputer.fit_transform(features_encoded), columns=features_encoded.columns)

# Step 6: Scale the features
scaler = StandardScaler()
features_scaled = pd.DataFrame(scaler.fit_transform(features_encoded_imputed), columns=features_encoded_imputed.columns)

# Step 7: Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(features_scaled, target)

# Step 8: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Step 9: Define multiple models with hyperparameter tuning using GridSearchCV
param_grids = {
    'GradientBoosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'learning_rate': [0.01, 0.1, 0.2],
            'max_depth': [3, 5, 7, 9],
            'subsample': [0.8, 1.0]
        }
    },
    'RandomForest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'n_estimators': [100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5, 10],
            'max_features': ['sqrt', 'log2']
        }
    },
    'LogisticRegression': {
        'model': LogisticRegression(random_state=42, max_iter=1000),
        'params': {
            'C': [0.01, 0.1, 1, 10, 100],
            'solver': ['lbfgs', 'liblinear', 'saga']
        }
    },
    'SVC': {
        'model': SVC(random_state=42),
        'params': {
            'C': [0.01, 0.1, 1, 10, 100],
            'kernel': ['linear', 'rbf'],
            'gamma': ['scale', 'auto']
        }
    }
}

# Initialize a dictionary to store the best models
best_models = {}

# Stratified K-Fold cross-validation
cv = StratifiedKFold(n_splits=5)

# Loop through each model and perform GridSearchCV
for model_name, model_info in param_grids.items():
    print(f'Training {model_name}...')
    grid_search = GridSearchCV(estimator=model_info['model'], param_grid=model_info['params'], cv=cv, n_jobs=-1, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f'Best Parameters for {model_name}: {grid_search.best_params_}')

# Step 10: Evaluate each model and choose the best one based on test set accuracy
model_accuracies = {}

for model_name, model in best_models.items():
    print(f'Evaluating {model_name}...')
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    model_accuracies[model_name] = accuracy
    report = classification_report(y_test, y_pred)

    # Display model results
    print(f'{model_name} Accuracy: {accuracy:.2f}')
    print(f'{model_name} Classification Report:')
    print(report)

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)

    # Plotting Confusion Matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix for {model_name}')
    plt.ylabel('Actual Class')
    plt.xlabel('Predicted Class')
    plt.show()

# Step 11: Choose the best model based on accuracy
best_model_name = max(model_accuracies, key=model_accuracies.get)
best_model = best_models[best_model_name]
print(f'Best Model: {best_model_name} with Accuracy: {model_accuracies[best_model_name]:.2f}')

# Step 12: Make predictions for all users with the best model
data['Predicted_MH_Encoded'] = best_model.predict(features_scaled)

# Step 13: Identify and provide personalized feedback to high-risk users
high_risk_users = data[data['Predicted_MH_Encoded'] == 1]
print("High-Risk Users Identified")

# Directory to save feedback files
feedback_dir = 'high_risk_feedback'
os.makedirs(feedback_dir, exist_ok=True)

# Feedback mechanism for high-risk users based on personalized factors
for index, row in high_risk_users.iterrows():
    player_id = row['Player_ID']
    game_time = row['Total_Game_Time']  # Assuming there is a 'Total_Game_Time' column
    game_name = row['Game_Name']
    platform = row['Platform']

    feedback = f"Player ID: {player_id}, based on your gaming habits, we recommend the following:\n"

    # Personalized feedback based on total game time
    if game_time > 5:
        feedback += f"Your total game time on {game_name} is quite high. Consider taking regular breaks to manage stress and maintain your mental health.\n"
    else:
        feedback += f"Your gaming time on {game_name} is moderate, but it's still important to stay mindful of your mental health.\n"

    # Additional feedback based on platform
    if platform == 'Mobile':
        feedback += "As a mobile gamer, try to limit screen time and avoid playing in poor lighting conditions.\n"
    elif platform == 'PC':
        feedback += "As a PC gamer, ensure you're taking care of your posture and sitting ergonomically to avoid strain.\n"

    # General mental health recommendation
    feedback += "If you feel overwhelmed, consider talking to a professional or engaging in relaxing activities.\n"

    # Save the feedback to a separate CSV file for each user
    feedback_file_path = os.path.join(feedback_dir, f'feedback_player_{player_id}.csv')
    feedback_df = pd.DataFrame([[player_id, feedback]], columns=['Player_ID', 'Feedback'])
    feedback_df.to_csv(feedback_file_path, index=False)

    print(f"Feedback for Player {player_id} saved to {feedback_file_path}")

# Optional: Save the list of high-risk users to a single CSV file
high_risk_users.to_csv('high_risk_users.csv', index=False)
